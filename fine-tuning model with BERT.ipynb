{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-17T13:08:05.470321200Z",
     "start_time": "2024-04-17T13:07:51.331283400Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AdamW, AutoTokenizer, AutoModelForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def load_data(file_path):\n",
    "    with open(file_path, \"r\", encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "    return data\n",
    "\n",
    "def extract_dialog(dialog, start_percentage, end_percentage):\n",
    "    if isinstance(dialog, list):\n",
    "        seeker_contents = [item['content'] for item in dialog if item['speaker'] == 'seeker']\n",
    "        start_index = int(start_percentage * len(seeker_contents))\n",
    "        end_index = int(end_percentage * len(seeker_contents))\n",
    "        return ' '.join(seeker_contents[start_index:end_index])\n",
    "    elif isinstance(dialog, str):\n",
    "        sentences = dialog.split('.')\n",
    "        seeker_contents = [sentence for sentence in sentences] #[str, str, ..., str]\n",
    "        start_index = int(start_percentage * len(seeker_contents))\n",
    "        end_index = int(end_percentage * len(seeker_contents))\n",
    "        return seeker_contents[start_index:end_index]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def train_test_split_part_dialog(dialog, labels, start_percentage, end_percentage, test_start, test_end):\n",
    "    train_data = []\n",
    "    test_data = []\n",
    "\n",
    "    if isinstance(labels, pd.Series):\n",
    "        labels = labels.tolist()\n",
    "\n",
    "    # Ensure labels are within the range of 0 to n_classes - 1\n",
    "    min_label = min(labels)\n",
    "    train_labels = labels\n",
    "    test_labels = labels\n",
    "\n",
    "    if isinstance(dialog, list):\n",
    "        for conv in dialog:\n",
    "            seeker_contents = [item['content'] for item in conv if item['speaker'] == 'seeker']\n",
    "            start_index = int(start_percentage * len(seeker_contents))\n",
    "            end_index = int(end_percentage * len(seeker_contents))\n",
    "            test_start_index = int(test_start * len(seeker_contents))\n",
    "            test_end_index = int(test_end * len(seeker_contents))\n",
    "\n",
    "            train_data.extend(seeker_contents[start_index:end_index])\n",
    "            test_data.extend(seeker_contents[test_start_index:test_end_index])\n",
    "\n",
    "    elif isinstance(dialog, str):\n",
    "        sentences = dialog.split('.')\n",
    "        seeker_contents = [sentence.strip() for sentence in sentences]\n",
    "        start_index = int(start_percentage * len(seeker_contents))\n",
    "        end_index = int(end_percentage * len(seeker_contents))\n",
    "        test_start_index = int(test_start * len(seeker_contents))\n",
    "        test_end_index = int(test_end * len(seeker_contents))\n",
    "\n",
    "        train_data = seeker_contents[start_index:end_index]\n",
    "        test_data = seeker_contents[test_start_index:test_end_index]\n",
    "\n",
    "    elif isinstance(dialog, pd.Series):\n",
    "        for conv in dialog:\n",
    "            seeker_contents = conv\n",
    "            start_index = int(start_percentage * len(seeker_contents))\n",
    "            end_index = int(end_percentage * len(seeker_contents))\n",
    "            test_start_index = int(test_start * len(seeker_contents))\n",
    "            test_end_index = int(test_end * len(seeker_contents))\n",
    "\n",
    "            # train_data.append((seeker_contents[start_index:end_index],))  # Append as tuple\n",
    "            # test_data.append((seeker_contents[test_start_index:test_end_index],))  # Append as tuple\n",
    "            train_data.append(' '.join(seeker_contents[start_index:end_index]))\n",
    "            test_data.append(' '.join(seeker_contents[test_start_index:test_end_index]))\n",
    "\n",
    "    return tuple(train_data), tuple(test_data), train_labels, test_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "dataset = load_data(\"ESConv.json\")\n",
    "dataframe = pd.DataFrame(dataset)\n",
    "\n",
    "df = pd.DataFrame(dataset)\n",
    "\n",
    "df['dialog'] = dataframe['dialog'].apply(lambda x: extract_dialog(x, 0, 1)) #take whole dialog from seeker\n",
    "df['dialog'] = df['dialog'].apply(lambda x: ' '.join(x) if isinstance(x, list) else x)\n",
    "\n",
    "df['initial_emotion_intensity'] = dataframe['survey_score'].apply(\n",
    "    lambda x: x['seeker']['initial_emotion_intensity'])\n",
    "df['initial_emotion_intensity'].dropna(inplace=True)\n",
    "df['initial_emotion_intensity'] = df['initial_emotion_intensity'].astype(int)\n",
    "df['dialog'].dropna(inplace=True)\n",
    "\n",
    "train_data, test_data, train_labels, test_labels = train_test_split_part_dialog(\n",
    "    df['dialog'], df['initial_emotion_intensity'],\n",
    "    start_percentage=0, end_percentage=0.2,\n",
    "    test_start=0.2, test_end=0.4)\n",
    "\n",
    "# Podział na dane treningowe, testowe i walidacyjne\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(train_data, train_labels, test_size=0.2, random_state=42)\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(train_texts, train_labels, test_size=0.1, random_state=42)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-17T13:08:30.536534100Z",
     "start_time": "2024-04-17T13:08:26.590043200Z"
    }
   },
   "id": "d07a90872340538b",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "tf_model.h5:   0%|          | 0.00/501M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3a3e8b1b32ef4db8b33a4f34744367ff"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\envs\\SupportSystem\\lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Admin\\.cache\\huggingface\\hub\\models--arpanghoshal--EmoRoBERTa. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at arpanghoshal/EmoRoBERTa.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_9336\\2484894677.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_labels = torch.tensor(train_labels)\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_9336\\2484894677.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  val_labels = torch.tensor(val_labels)\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_9336\\2484894677.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_labels = torch.tensor(test_labels)\n"
     ]
    }
   ],
   "source": [
    "# Inicjalizacja modelu BERT i tokenizer'a\n",
    "from transformers import RobertaTokenizerFast, TFRobertaForSequenceClassification, pipeline\n",
    "checkpoint_EmoRoberta = \"arpanghoshal/EmoRoBERTa\"\n",
    "# checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint_EmoRoberta)\n",
    "#model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "model = TFRobertaForSequenceClassification.from_pretrained(checkpoint_EmoRoberta)\n",
    "\n",
    "# Tokenizacja danych\n",
    "train_encodings = tokenizer(train_texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "val_encodings = tokenizer(val_texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "test_encodings = tokenizer(test_texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# Dodanie etykiet do danych treningowych\n",
    "train_labels = torch.tensor(train_labels)\n",
    "val_labels = torch.tensor(val_labels)\n",
    "test_labels = torch.tensor(test_labels)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-17T13:19:29.045758800Z",
     "start_time": "2024-04-17T13:18:18.017420100Z"
    }
   },
   "id": "5bd2cacca26dcc68",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "ResourceExhaustedError",
     "evalue": "Exception encountered when calling layer 'self' (type TFRobertaSelfAttention).\n\n{{function_node __wrapped__RealDiv_device_/job:localhost/replica:0/task:0/device:CPU:0}} OOM when allocating tensor with shape[32,12,512,512] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu [Op:RealDiv] name: \n\nCall arguments received by layer 'self' (type TFRobertaSelfAttention):\n  • hidden_states=tf.Tensor(shape=(32, 512, 768), dtype=float32)\n  • attention_mask=tf.Tensor(shape=(32, 1, 1, 512), dtype=float32)\n  • head_mask=None\n  • encoder_hidden_states=None\n  • encoder_attention_mask=None\n  • past_key_value=None\n  • output_attentions=False\n  • training=False",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mResourceExhaustedError\u001B[0m                    Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[8], line 26\u001B[0m\n\u001B[0;32m     24\u001B[0m inputs, labels \u001B[38;5;241m=\u001B[39m batch\n\u001B[0;32m     25\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m tf\u001B[38;5;241m.\u001B[39mGradientTape() \u001B[38;5;28;01mas\u001B[39;00m tape:\n\u001B[1;32m---> 26\u001B[0m     logits \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m     27\u001B[0m     loss \u001B[38;5;241m=\u001B[39m loss_fn(labels, logits)\n\u001B[0;32m     28\u001B[0m gradients \u001B[38;5;241m=\u001B[39m tape\u001B[38;5;241m.\u001B[39mgradient(loss, model\u001B[38;5;241m.\u001B[39mtrainable_variables)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\SupportSystem\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     67\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[0;32m     68\u001B[0m     \u001B[38;5;66;03m# To get the full stack trace, call:\u001B[39;00m\n\u001B[0;32m     69\u001B[0m     \u001B[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001B[39;00m\n\u001B[1;32m---> 70\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m     71\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m     72\u001B[0m     \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\SupportSystem\\lib\\site-packages\\transformers\\modeling_tf_utils.py:428\u001B[0m, in \u001B[0;36munpack_inputs.<locals>.run_call_with_unpacked_inputs\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m    425\u001B[0m     config \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\n\u001B[0;32m    427\u001B[0m unpacked_inputs \u001B[38;5;241m=\u001B[39m input_processing(func, config, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mfn_args_and_kwargs)\n\u001B[1;32m--> 428\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43munpacked_inputs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\SupportSystem\\lib\\site-packages\\transformers\\models\\roberta\\modeling_tf_roberta.py:1441\u001B[0m, in \u001B[0;36mTFRobertaForSequenceClassification.call\u001B[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict, labels, training)\u001B[0m\n\u001B[0;32m   1412\u001B[0m \u001B[38;5;129m@unpack_inputs\u001B[39m\n\u001B[0;32m   1413\u001B[0m \u001B[38;5;129m@add_start_docstrings_to_model_forward\u001B[39m(ROBERTA_INPUTS_DOCSTRING\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbatch_size, sequence_length\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n\u001B[0;32m   1414\u001B[0m \u001B[38;5;129m@add_code_sample_docstrings\u001B[39m(\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1433\u001B[0m     training: Optional[\u001B[38;5;28mbool\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[0;32m   1434\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Union[TFSequenceClassifierOutput, Tuple[tf\u001B[38;5;241m.\u001B[39mTensor]]:\n\u001B[0;32m   1435\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   1436\u001B[0m \u001B[38;5;124;03m    labels (`tf.Tensor` of shape `(batch_size,)`, *optional*):\u001B[39;00m\n\u001B[0;32m   1437\u001B[0m \u001B[38;5;124;03m        Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001B[39;00m\n\u001B[0;32m   1438\u001B[0m \u001B[38;5;124;03m        config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001B[39;00m\n\u001B[0;32m   1439\u001B[0m \u001B[38;5;124;03m        `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001B[39;00m\n\u001B[0;32m   1440\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m-> 1441\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mroberta\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1442\u001B[0m \u001B[43m        \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1443\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1444\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtoken_type_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtoken_type_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1445\u001B[0m \u001B[43m        \u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mposition_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1446\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1447\u001B[0m \u001B[43m        \u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs_embeds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1448\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1449\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1450\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1451\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtraining\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtraining\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1452\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1453\u001B[0m     sequence_output \u001B[38;5;241m=\u001B[39m outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m   1454\u001B[0m     logits \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclassifier(sequence_output, training\u001B[38;5;241m=\u001B[39mtraining)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\SupportSystem\\lib\\site-packages\\transformers\\modeling_tf_utils.py:428\u001B[0m, in \u001B[0;36munpack_inputs.<locals>.run_call_with_unpacked_inputs\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m    425\u001B[0m     config \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\n\u001B[0;32m    427\u001B[0m unpacked_inputs \u001B[38;5;241m=\u001B[39m input_processing(func, config, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mfn_args_and_kwargs)\n\u001B[1;32m--> 428\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43munpacked_inputs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\SupportSystem\\lib\\site-packages\\transformers\\models\\roberta\\modeling_tf_roberta.py:836\u001B[0m, in \u001B[0;36mTFRobertaMainLayer.call\u001B[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, training)\u001B[0m\n\u001B[0;32m    833\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    834\u001B[0m     head_mask \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28;01mNone\u001B[39;00m] \u001B[38;5;241m*\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mnum_hidden_layers\n\u001B[1;32m--> 836\u001B[0m encoder_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencoder\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    837\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43membedding_output\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    838\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mextended_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    839\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    840\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    841\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoder_extended_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    842\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpast_key_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    843\u001B[0m \u001B[43m    \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    844\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    845\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    846\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    847\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtraining\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtraining\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    848\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    850\u001B[0m sequence_output \u001B[38;5;241m=\u001B[39m encoder_outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m    851\u001B[0m pooled_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpooler(hidden_states\u001B[38;5;241m=\u001B[39msequence_output) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpooler \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\SupportSystem\\lib\\site-packages\\transformers\\models\\roberta\\modeling_tf_roberta.py:625\u001B[0m, in \u001B[0;36mTFRobertaEncoder.call\u001B[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, training)\u001B[0m\n\u001B[0;32m    621\u001B[0m     all_hidden_states \u001B[38;5;241m=\u001B[39m all_hidden_states \u001B[38;5;241m+\u001B[39m (hidden_states,)\n\u001B[0;32m    623\u001B[0m past_key_value \u001B[38;5;241m=\u001B[39m past_key_values[i] \u001B[38;5;28;01mif\u001B[39;00m past_key_values \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m--> 625\u001B[0m layer_outputs \u001B[38;5;241m=\u001B[39m \u001B[43mlayer_module\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    626\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    627\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    628\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhead_mask\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    629\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    630\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    631\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpast_key_value\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpast_key_value\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    632\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    633\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtraining\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtraining\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    634\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    635\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m layer_outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m    637\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m use_cache:\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\SupportSystem\\lib\\site-packages\\transformers\\models\\roberta\\modeling_tf_roberta.py:517\u001B[0m, in \u001B[0;36mTFRobertaLayer.call\u001B[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions, training)\u001B[0m\n\u001B[0;32m    504\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcall\u001B[39m(\n\u001B[0;32m    505\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    506\u001B[0m     hidden_states: tf\u001B[38;5;241m.\u001B[39mTensor,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    514\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[tf\u001B[38;5;241m.\u001B[39mTensor]:\n\u001B[0;32m    515\u001B[0m     \u001B[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001B[39;00m\n\u001B[0;32m    516\u001B[0m     self_attn_past_key_value \u001B[38;5;241m=\u001B[39m past_key_value[:\u001B[38;5;241m2\u001B[39m] \u001B[38;5;28;01mif\u001B[39;00m past_key_value \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m--> 517\u001B[0m     self_attention_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mattention\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    518\u001B[0m \u001B[43m        \u001B[49m\u001B[43minput_tensor\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    519\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    520\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    521\u001B[0m \u001B[43m        \u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    522\u001B[0m \u001B[43m        \u001B[49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    523\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpast_key_value\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mself_attn_past_key_value\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    524\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    525\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtraining\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtraining\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    526\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    527\u001B[0m     attention_output \u001B[38;5;241m=\u001B[39m self_attention_outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m    529\u001B[0m     \u001B[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\SupportSystem\\lib\\site-packages\\transformers\\models\\roberta\\modeling_tf_roberta.py:398\u001B[0m, in \u001B[0;36mTFRobertaAttention.call\u001B[1;34m(self, input_tensor, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions, training)\u001B[0m\n\u001B[0;32m    387\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcall\u001B[39m(\n\u001B[0;32m    388\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    389\u001B[0m     input_tensor: tf\u001B[38;5;241m.\u001B[39mTensor,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    396\u001B[0m     training: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[0;32m    397\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[tf\u001B[38;5;241m.\u001B[39mTensor]:\n\u001B[1;32m--> 398\u001B[0m     self_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mself_attention\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    399\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minput_tensor\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    400\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    401\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    402\u001B[0m \u001B[43m        \u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    403\u001B[0m \u001B[43m        \u001B[49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    404\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpast_key_value\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpast_key_value\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    405\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    406\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtraining\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtraining\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    407\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    408\u001B[0m     attention_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdense_output(\n\u001B[0;32m    409\u001B[0m         hidden_states\u001B[38;5;241m=\u001B[39mself_outputs[\u001B[38;5;241m0\u001B[39m], input_tensor\u001B[38;5;241m=\u001B[39minput_tensor, training\u001B[38;5;241m=\u001B[39mtraining\n\u001B[0;32m    410\u001B[0m     )\n\u001B[0;32m    411\u001B[0m     \u001B[38;5;66;03m# add attentions (possibly with past_key_value) if we output them\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\SupportSystem\\lib\\site-packages\\transformers\\models\\roberta\\modeling_tf_roberta.py:302\u001B[0m, in \u001B[0;36mTFRobertaSelfAttention.call\u001B[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions, training)\u001B[0m\n\u001B[0;32m    300\u001B[0m attention_scores \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mmatmul(query_layer, key_layer, transpose_b\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m    301\u001B[0m dk \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mcast(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msqrt_att_head_size, dtype\u001B[38;5;241m=\u001B[39mattention_scores\u001B[38;5;241m.\u001B[39mdtype)\n\u001B[1;32m--> 302\u001B[0m attention_scores \u001B[38;5;241m=\u001B[39m \u001B[43mtf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdivide\u001B[49m\u001B[43m(\u001B[49m\u001B[43mattention_scores\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdk\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    304\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m attention_mask \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    305\u001B[0m     \u001B[38;5;66;03m# Apply the attention mask is (precomputed for all layers in TFRobertaModel call() function)\u001B[39;00m\n\u001B[0;32m    306\u001B[0m     attention_scores \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39madd(attention_scores, attention_mask)\n",
      "\u001B[1;31mResourceExhaustedError\u001B[0m: Exception encountered when calling layer 'self' (type TFRobertaSelfAttention).\n\n{{function_node __wrapped__RealDiv_device_/job:localhost/replica:0/task:0/device:CPU:0}} OOM when allocating tensor with shape[32,12,512,512] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu [Op:RealDiv] name: \n\nCall arguments received by layer 'self' (type TFRobertaSelfAttention):\n  • hidden_states=tf.Tensor(shape=(32, 512, 768), dtype=float32)\n  • attention_mask=tf.Tensor(shape=(32, 1, 1, 512), dtype=float32)\n  • head_mask=None\n  • encoder_hidden_states=None\n  • encoder_attention_mask=None\n  • past_key_value=None\n  • output_attentions=False\n  • training=False"
     ]
    }
   ],
   "source": [
    "# Trening modelu \"arpanghoshal/EmoRoBERTa\"\n",
    "import tensorflow as tf\n",
    "\n",
    "# Definicja hiperparametrów\n",
    "epochs = 3\n",
    "batch_size = 32\n",
    "learning_rate = 1e-5\n",
    "\n",
    "# Definicja optymalizatora\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "# Przygotowanie danych treningowych w formacie TensorFlow Dataset\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(train_encodings),\n",
    "    train_labels\n",
    ")).shuffle(len(train_encodings)).batch(batch_size)\n",
    "\n",
    "# Definicja funkcji straty\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# Pętla treningowa\n",
    "for epoch in range(epochs):\n",
    "    for batch in train_dataset:\n",
    "        inputs, labels = batch\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = model(inputs)[0]\n",
    "            loss = loss_fn(labels, logits)\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    # Obliczenie straty dla danych treningowych na końcu epoki\n",
    "    train_loss = 0\n",
    "    for batch in train_dataset:\n",
    "        inputs, labels = batch\n",
    "        logits = model(inputs)[0]\n",
    "        train_loss += loss_fn(labels, logits)\n",
    "    train_loss /= len(train_dataset)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Train Loss: {train_loss}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-17T13:22:54.822608100Z",
     "start_time": "2024-04-17T13:22:19.092955600Z"
    }
   },
   "id": "de5e1c3db3bcc01a",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TFRobertaForSequenceClassification' object has no attribute 'train'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[7], line 5\u001B[0m\n\u001B[0;32m      3\u001B[0m epochs \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m3\u001B[39m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(epochs):\n\u001B[1;32m----> 5\u001B[0m     \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m()\n\u001B[0;32m      6\u001B[0m     optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m      7\u001B[0m     train_loss \u001B[38;5;241m=\u001B[39m model(input_ids\u001B[38;5;241m=\u001B[39mtrain_encodings[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124minput_ids\u001B[39m\u001B[38;5;124m'\u001B[39m], attention_mask\u001B[38;5;241m=\u001B[39mtrain_encodings[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mattention_mask\u001B[39m\u001B[38;5;124m'\u001B[39m], labels\u001B[38;5;241m=\u001B[39mtrain_labels)\u001B[38;5;241m.\u001B[39mloss\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'TFRobertaForSequenceClassification' object has no attribute 'train'"
     ]
    }
   ],
   "source": [
    "# Trening modelu \"bert-base-uncased\"\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    train_loss = model(input_ids=train_encodings['input_ids'], attention_mask=train_encodings['attention_mask'], labels=train_labels).loss\n",
    "    train_loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}: Train loss {train_loss}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-17T13:19:59.210775100Z",
     "start_time": "2024-04-17T13:19:59.112171400Z"
    }
   },
   "id": "a63e4d16e3fc73ab",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Testowanie modelu\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    val_loss = model(input_ids=val_encodings['input_ids'], attention_mask=val_encodings['attention_mask'], labels=val_labels).loss\n",
    "    print(f\"Validation loss: {val_loss}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f988e77fcf0997ae"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Ocena na zbiorze testowym\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids=test_encodings['input_ids'], attention_mask=test_encodings['attention_mask'])\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=1)\n",
    "    accuracy = (predictions == test_labels).float().mean()\n",
    "    print(f\"Accuracy on test set: {accuracy.item()}\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "26883f6f9a767434"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
